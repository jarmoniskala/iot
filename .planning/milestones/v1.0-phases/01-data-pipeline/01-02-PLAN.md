---
phase: 01-data-pipeline
plan: 02
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - supabase/functions/poll-fmi/index.ts
  - supabase/migrations/003_cron_jobs.sql
autonomous: true
requirements:
  - PIPE-03
  - PIPE-04
  - PIPE-05
  - PIPE-06

must_haves:
  truths:
    - "FMI weather observations for Helsinki-Vantaa airport (FMISID 100968) are fetched, parsed, and stored in the weather_observations table"
    - "The FMI XML response is correctly parsed -- all 13 parameters (temperature, wind speed, wind gust, wind direction, humidity, dew point, precipitation 1h, precipitation intensity, snow depth, pressure, visibility, cloud cover, weather code) are extracted and stored in their respective columns"
    - "NaN values in FMI response are stored as NULL in the database (not as the string 'NaN' or the number NaN)"
    - "Duplicate FMI observations (same fmisid + observed_at) are handled without error"
    - "Failed FMI fetches are logged to ingestion_log with source='fmi' and status='error' for Phase 3 health view"
    - "pg_cron job invokes the poll-fmi edge function every 10 minutes via pg_net"
    - "The pg_cron polling provides continuous database activity that prevents 7-day Supabase project pause"
    - "A daily pg_cron job creates next month's partitions for both sensor_readings and weather_observations"
    - "Database storage size is queryable via get_database_size_mb() function (created in Plan 01)"
  artifacts:
    - path: "supabase/functions/poll-fmi/index.ts"
      provides: "FMI weather observation fetcher and XML parser"
      exports: ["Deno.serve"]
      min_lines: 80
    - path: "supabase/migrations/003_cron_jobs.sql"
      provides: "pg_cron schedules for FMI polling, partition maintenance, and Vault secrets setup"
      contains: "cron.schedule"
  key_links:
    - from: "supabase/functions/poll-fmi/index.ts"
      to: "weather_observations table"
      via: "supabase.from('weather_observations').upsert()"
      pattern: "from.*weather_observations.*(insert|upsert)"
    - from: "supabase/functions/poll-fmi/index.ts"
      to: "ingestion_log table"
      via: "supabase.from('ingestion_log').insert() on success and failure"
      pattern: "from.*ingestion_log.*insert"
    - from: "supabase/migrations/003_cron_jobs.sql"
      to: "supabase/functions/poll-fmi/index.ts"
      via: "pg_net HTTP POST to edge function URL"
      pattern: "net.http_post"
    - from: "supabase/migrations/003_cron_jobs.sql"
      to: "create_monthly_partition function"
      via: "pg_cron daily job calls partition function"
      pattern: "create_monthly_partition"
---

<objective>
Create the FMI weather polling edge function and set up all pg_cron scheduled jobs for automated operation.

Purpose: Complete the data pipeline by adding automatic weather data collection and infrastructure reliability -- after this plan, the system runs autonomously 24/7 collecting both indoor sensor data (via Plan 01's edge function) and outdoor weather data, with automatic partition management and built-in keep-alive.

Output: A Deno edge function that fetches and parses FMI XML weather data, and a SQL migration that configures pg_cron jobs for FMI polling (every 10 min), partition creation (daily), plus Vault secrets for secure edge function invocation.
</objective>

<execution_context>
@/Users/jarmoniskala/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jarmoniskala/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-pipeline/01-CONTEXT.md
@.planning/phases/01-data-pipeline/01-RESEARCH.md
@.planning/phases/01-data-pipeline/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: FMI weather polling edge function</name>
  <files>supabase/functions/poll-fmi/index.ts</files>
  <action>
Create the Deno edge function that fetches weather observations from the FMI WFS API and stores them in the weather_observations table.

**Imports:**
```typescript
import { createClient } from "npm:@supabase/supabase-js@2";
import { XMLParser } from "npm:fast-xml-parser@4.4.1";
```

**FMI API URL:**
```
https://opendata.fmi.fi/wfs?service=WFS&version=2.0.0&request=getFeature&storedquery_id=fmi::observations::weather::multipointcoverage&fmisid=100968&timestep=10
```

This returns the latest observations (default: last 12 hours) for Helsinki-Vantaa airport with 10-minute intervals.

**Handler structure (Deno.serve):**

1. **CORS handling:** Same pattern as ingest-sensors -- return 200 on OPTIONS.

2. **Initialize Supabase client** with `SUPABASE_URL` and `SUPABASE_SERVICE_ROLE_KEY`.

3. **Fetch FMI data** with explicit timeout (8 seconds) to avoid edge function timeout:
   ```typescript
   const controller = new AbortController();
   const timeoutId = setTimeout(() => controller.abort(), 8000);
   const response = await fetch(FMI_URL, { signal: controller.signal });
   clearTimeout(timeoutId);
   ```

4. **Parse XML** using fast-xml-parser with `removeNSPrefix: true` to strip namespace prefixes:
   ```typescript
   const parser = new XMLParser({
     ignoreAttributes: false,
     removeNSPrefix: true,
   });
   const parsed = parser.parse(xmlText);
   ```

5. **Extract observation data** from the parsed XML. The critical path through the FMI WFS response:

   a. **Timestamps:** Navigate to `FeatureCollection > member > GridSeriesObservation > domainSet > SimpleMultiPoint > positions` (or similar nested path). This contains newline-separated entries with lat, lon, and Unix timestamp (seconds). Extract timestamps.

   b. **Parameter names (13 fields):** Navigate to `FeatureCollection > member > GridSeriesObservation > rangeSet > DataBlock > rangeType > DataRecord > field`. Each field has a `@_name` attribute. The 13 parameter names in order are: `t2m`, `ws_10min`, `wg_10min`, `wd_10min`, `rh`, `td`, `r_1h`, `ri_10min`, `snow_aws`, `p_sea`, `vis`, `n_man`, `wawa`.

   c. **Values:** Navigate to `FeatureCollection > member > GridSeriesObservation > rangeSet > DataBlock > doubleOrNilReasonTupleList`. This is a string with newline-separated rows, each row containing 13 space-separated values. Split by newline, then by whitespace.

   d. **Map values to columns:** For each timestamp row, zip the 13 values with their parameter names:
      - `t2m` -> `temperature`
      - `ws_10min` -> `wind_speed`
      - `wg_10min` -> `wind_gust`
      - `wd_10min` -> `wind_direction`
      - `rh` -> `humidity`
      - `td` -> `dew_point`
      - `r_1h` -> `precipitation_1h`
      - `ri_10min` -> `precipitation_intensity`
      - `snow_aws` -> `snow_depth`
      - `p_sea` -> `pressure`
      - `vis` -> `visibility`
      - `n_man` -> `cloud_cover`
      - `wawa` -> `weather_code`

   e. **Handle NaN values:** FMI uses the string `NaN` for missing data. Convert `NaN` to `null` when inserting into the database. Use: `parseFloat(val)` and check `Number.isNaN(result) ? null : result`.

   f. **Store raw values string** in `raw_values` column for each row (the original space-separated values before parsing).

6. **Insert observations** into weather_observations table. Only insert the LATEST observation(s) -- not the full 12 hours of historical data that FMI returns by default. To do this efficiently:
   - Parse all rows from the response
   - Take only observations from the last 20 minutes (to catch the latest 10-min observation even if slightly delayed)
   - Use `ON CONFLICT (fmisid, observed_at) DO NOTHING` for deduplication (this works correctly in PostgreSQL 15+ with partitioned tables when the unique constraint includes the partition key)

7. **Log results** to ingestion_log:
   - On success: `{ source: 'fmi', status: 'success', readings_count: N, duplicates_count: 0, details: { latestObservation: timestamp } }`
   - On fetch/parse error: `{ source: 'fmi', status: 'error', error_message: error.message, details: { url: FMI_URL } }`

8. **Response:** Return `{ ok: true, observations: N }` on success, `{ ok: false, error: "..." }` on failure.

**Important implementation notes:**
- The XML structure can be deeply nested and the exact path may vary. Build the parser defensively -- log the parsed structure on first run to verify paths. Include fallback error messages that describe what part of the XML was unexpected.
- Use `npm:` imports only. Do NOT use `esm.sh` or `deno.land/x`.
- FMI does NOT require an API key. No authorization header needed for the FMI fetch.
- Only store observations from the last ~20 minutes per invocation, not the full response. This prevents re-inserting old data on every poll.
  </action>
  <verify>
Review the edge function code for:
- Correct imports (`npm:@supabase/supabase-js@2`, `npm:fast-xml-parser@4.4.1`)
- FMI URL uses fmisid=100968 and timestep=10
- Fetch has an 8-second timeout via AbortController
- XMLParser configured with `removeNSPrefix: true`
- All 13 FMI parameters mapped to correct database columns
- NaN handling: parsed values checked with `Number.isNaN()`, converted to null
- Only recent observations inserted (not full 12-hour history)
- Deduplication via ON CONFLICT DO NOTHING
- Success and failure both logged to ingestion_log
- raw_values stored for each observation row
- No esm.sh or deno.land imports
  </verify>
  <done>
The poll-fmi edge function fetches FMI weather data for Helsinki-Vantaa airport, parses the WFS XML response using fast-xml-parser, extracts all 13 observation parameters, handles NaN values as nulls, inserts only recent observations with deduplication, and logs both successes and failures to the ingestion_log table.
  </done>
</task>

<task type="auto">
  <name>Task 2: pg_cron scheduled jobs and Vault secrets</name>
  <files>supabase/migrations/003_cron_jobs.sql</files>
  <action>
Create the SQL migration that sets up Vault secrets and all pg_cron scheduled jobs for autonomous operation.

**Enable required extensions (if not already enabled):**
```sql
CREATE EXTENSION IF NOT EXISTS pg_cron;
CREATE EXTENSION IF NOT EXISTS pg_net;
```

Note: These are typically pre-enabled on Supabase but include for safety.

**Vault secrets setup:**

Store the Supabase project URL and anon key in Vault for use by pg_net:
```sql
-- These will be populated by the user after deployment
-- Placeholder values that MUST be updated
SELECT vault.create_secret(
  'https://YOUR_PROJECT_REF.supabase.co',
  'project_url'
);
SELECT vault.create_secret(
  'YOUR_ANON_KEY',
  'anon_key'
);
```

Add clear SQL comments explaining the user must replace these values with their actual Supabase project URL and anon key. Include the exact location to find these values: "Supabase Dashboard -> Settings -> API -> Project URL and anon/public key."

**Job 1: FMI weather polling (every 10 minutes):**
```sql
SELECT cron.schedule(
  'poll-fmi-weather',
  '*/10 * * * *',
  $$
  SELECT net.http_post(
    url := (SELECT decrypted_secret FROM vault.decrypted_secrets WHERE name = 'project_url')
           || '/functions/v1/poll-fmi',
    headers := jsonb_build_object(
      'Content-Type', 'application/json',
      'Authorization', 'Bearer ' || (SELECT decrypted_secret FROM vault.decrypted_secrets WHERE name = 'anon_key')
    ),
    body := jsonb_build_object('invoked_at', now()::text),
    timeout_milliseconds := 10000
  ) AS request_id;
  $$
);
```

**Job 2: Monthly partition creation (daily at 03:00 UTC):**
```sql
SELECT cron.schedule(
  'create-monthly-partitions',
  '0 3 * * *',
  $$
  SELECT create_monthly_partition('sensor_readings', now()::date);
  SELECT create_monthly_partition('sensor_readings', (now() + interval '1 month')::date);
  SELECT create_monthly_partition('weather_observations', now()::date);
  SELECT create_monthly_partition('weather_observations', (now() + interval '1 month')::date);
  $$
);
```

This job is idempotent -- `create_monthly_partition` uses `CREATE TABLE IF NOT EXISTS`, so running it daily is safe.

**Job 3: Daily storage monitoring (daily at 06:00 UTC):**
```sql
SELECT cron.schedule(
  'log-storage-usage',
  '0 6 * * *',
  $$
  INSERT INTO ingestion_log (source, status, details)
  VALUES (
    'system',
    'storage_check',
    jsonb_build_object(
      'database_size_mb', (SELECT get_database_size_mb()),
      'checked_at', now()::text
    )
  );
  $$
);
```

This logs the daily database size to ingestion_log, satisfying PIPE-05 (storage monitoring). The Phase 2 dashboard can query this for display.

**Add comments throughout the migration** explaining:
- What each cron job does
- The schedule frequency and why
- That FMI polling doubles as keep-alive (PIPE-06)
- That Vault secrets must be updated before cron jobs will work
- How to verify jobs are running: `SELECT * FROM cron.job;` and `SELECT * FROM cron.job_run_details ORDER BY start_time DESC LIMIT 10;`
  </action>
  <verify>
Review the SQL migration for:
- Extensions enabled (pg_cron, pg_net)
- Vault secrets created with placeholder values and clear update instructions
- FMI polling cron: runs every 10 minutes (`*/10 * * * *`), calls poll-fmi edge function via pg_net
- Partition cron: runs daily at 03:00, creates current + next month for both tables
- Storage monitoring cron: runs daily at 06:00, logs size to ingestion_log
- All cron jobs use correct `cron.schedule()` syntax
- pg_net uses Vault secrets (not hardcoded keys)
- Comments explain each section and provide verification queries
  </verify>
  <done>
Three pg_cron jobs are configured: FMI polling every 10 minutes (also serving as keep-alive), daily partition creation for both partitioned tables, and daily storage usage logging. Vault secrets store the project URL and anon key for secure pg_net invocation. The migration includes clear instructions for the user to update Vault secrets with real values.
  </done>
</task>

</tasks>

<verification>
1. poll-fmi edge function correctly fetches, parses, and stores FMI weather observations with all 13 parameters
2. NaN values from FMI are stored as NULL, not as string or JS NaN
3. FMI fetch has timeout protection (8 seconds)
4. Both success and failure of FMI fetch are logged to ingestion_log
5. pg_cron job invokes poll-fmi every 10 minutes via pg_net
6. Daily partition creation job covers both sensor_readings and weather_observations
7. Storage monitoring logs database size daily
8. Vault secrets are set up with clear instructions for user to update
9. FMI polling activity prevents Supabase 7-day inactivity pause
</verification>

<success_criteria>
- poll-fmi edge function parses FMI WFS XML correctly, extracting all 13 weather parameters for FMISID 100968
- NaN values become NULL in database
- pg_cron schedules: FMI every 10 min, partitions daily, storage daily
- Vault secrets created with placeholder values and update instructions
- FMI polling serves as keep-alive (no additional keep-alive mechanism needed)
- Storage usage is queryable via get_database_size_mb() and logged daily
- All failures logged to ingestion_log for Phase 3 health view
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline/01-02-SUMMARY.md`
</output>
